{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b44028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 01:11:27.465770: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-17 01:11:27.465794: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-17 01:11:29.107575: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-17 01:11:29.107604: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-17 01:11:29.107626: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mingkaichen-Macmini): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "\n",
    "import dataset\n",
    "import model\n",
    "import traintest\n",
    "import telemetry\n",
    "import common.training as training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83030ff5-602b-4b78-be3e-f24be1260214",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47c3d23-28bd-4acf-aab6-30be1c88e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"tmp/4_persentence_pretrain_mlm_15p.log\", \n",
    "                    format='%(asctime)s %(message)s', \n",
    "                    filemode='w')\n",
    "logger=logging.getLogger() \n",
    "logger.setLevel(logging.DEBUG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ed0418-6cdc-43b6-a1cd-d1dde4baea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/pretrain_1.yaml\") as f:\n",
    "    _args = yaml.safe_load(f.read())\n",
    "    tokenizer_filename = _args[\"tokenizer_model\"]\n",
    "    training_args = _args[\"training_args\"]\n",
    "    model_args = _args[\"model_args\"]\n",
    "    training_settings = _args[\"training_settings\"]\n",
    "\n",
    "with open(\"configs/ps_dataset_1.yaml\") as f:\n",
    "    _args = yaml.safe_load(f.read())\n",
    "    dataset_path = _args[\"dataset\"]\n",
    "    dataset_shard_dirpath = _args[\"shard_directory\"]\n",
    "    tokenizer_setup = _args[\"tokenizer_args\"]\n",
    "\n",
    "with open(tokenizer_filename, 'rb') as f:\n",
    "    tokenizer = text.SentencepieceTokenizer(model=f.read(), \n",
    "                                            out_type=tf.int32, \n",
    "                                            add_bos=tokenizer_setup[\"add_bos\"], \n",
    "                                            add_eos=tokenizer_setup[\"add_eos\"])\n",
    "    \n",
    "# generate or retrieve cached values\n",
    "cached_args = dataset.cache_values(\"configs/pretrain_cache.yaml\", { \n",
    "    \"dataset_width\": dataset.generate_dataset_width,\n",
    "    \"vocab_size\": lambda _, tokenizer: int(tokenizer.vocab_size().numpy()),\n",
    "}, dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68c618",
   "metadata": {},
   "source": [
    "# Dataset Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ec6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = (training_args[\"batch_size\"], cached_args[\"dataset_width\"])\n",
    "latent_shape = (\n",
    "    training_args[\"batch_size\"], \n",
    "    model_args[\"latent_dim\"], \n",
    "    cached_args[\"dataset_width\"],\n",
    ")\n",
    "training_shards, training_keys = dataset.setup_shards(dataset_shard_dirpath, training_args,\n",
    "                                                      tokenizer, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b34b7",
   "metadata": {},
   "source": [
    "# Model and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4135318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = training.CustomSchedule(model_args[\"model_dim\"])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "_builder = model.InitParamBuilder()\n",
    "for param in cached_args:\n",
    "    _builder.add_param(cached_args[param], param)\n",
    "for param in model_args:\n",
    "    _builder.add_param(model_args[param], param) \n",
    "_args = _builder.build()\n",
    "\n",
    "pretrainer = model.PretrainerMLM(\n",
    "    tokenizer=tokenizer,\n",
    "    params=_args,\n",
    "    metadata=model.PretrainerMLMMetadataBuilder().\\\n",
    "        tokenizer_meta(tokenizer_filename).\\\n",
    "        optimizer_iter(optimizer.iterations).build())\n",
    "run_test = traintest.build_tester(pretrainer,\n",
    "                                  samples=[\n",
    "                                      \"este é um problema que temos que resolver.\", \n",
    "                                      \"this is a problem we have to solve .\", \n",
    "                                      \"os meus vizinhos ouviram sobre esta ideia.\", \n",
    "                                      \"and my neighboring homes heard about this idea .\", \n",
    "                                      \"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\", \n",
    "                                      \"so i \\'ll just share with you some stories very quickly of some magical things that have happened .\", \n",
    "                                      \"este é o primeiro livro que eu fiz.\", \n",
    "                                      \"this is the first book i've ever done.\",\n",
    "                                  ],\n",
    "                                  mask_sizes=batch_shape,\n",
    "                                  logger=logger)\n",
    "train_batch, train_loss, train_accuracy = traintest.build_pretrainer(pretrainer, optimizer, batch_shape)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(pretrainer=pretrainer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                          directory=\"tmp/checkpoints/train_15p_ps\",\n",
    "                                          max_to_keep=50)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    logger.info('Latest checkpoint restored!!')\n",
    "\n",
    "QUOTA_BUCKET_CAPACITY = 50\n",
    "QUOTA_BUCKET_RECOVER = 10\n",
    "QUOTA_BUCKET_RECOVER_RATE = 50\n",
    "SKIP_LOSS_WINDOW = 200\n",
    "\n",
    "bucket = training.QuotaBucket(training_settings[\"skip_bad_loss\"][\"warmup\"],\n",
    "                            bucket_capacity=QUOTA_BUCKET_CAPACITY,\n",
    "                            bucket_recover=QUOTA_BUCKET_RECOVER,\n",
    "                            bucket_recover_rate=QUOTA_BUCKET_RECOVER_RATE)\n",
    "prev_losses = training.LossWindow(capacity=SKIP_LOSS_WINDOW)\n",
    "nan_reporter = telemetry.tokens_reporter(logger, tokenizer)\n",
    "\n",
    "def run_epochs(epochs, skip_shards=None):\n",
    "    \"\"\" run all epoch \"\"\"\n",
    "    trainer = traintest.EpochPretrainer(\n",
    "        traintest.EpochPretrainerInitBuilder().\\\n",
    "            training_settings(training_settings).\\\n",
    "            training_loss(train_loss).\\\n",
    "            training_accuracy(train_accuracy).\\\n",
    "            training_shards(training_shards).\\\n",
    "            training_cb(lambda batch, lengths, loss_check: train_batch(\n",
    "                batch, lengths, loss_check,\n",
    "                mask_rate=training_args[\"mask_rate\"],\n",
    "                context_rate=training_args[\"context_rate\"])).\\\n",
    "            ckpt_manager(ckpt_manager).\\\n",
    "            bucket(bucket).\\\n",
    "            prev_losses(prev_losses).build())\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        sublogger = telemetry.PrefixAdapter(logger, 'Epoch {}'.format(epoch+1))\n",
    "        trainer.run_epoch(skip_shards, logger=sublogger, nan_reporter=nan_reporter)\n",
    "\n",
    "        if (epoch + 1) % training_settings[\"epochs_per_save\"] == 0:\n",
    "            logger.info('Saving checkpoint for epoch %d at %s', epoch+1, ckpt_manager.save())\n",
    "\n",
    "        if (epoch + 1) % training_settings[\"epochs_per_test\"] == 0:\n",
    "            run_test() # run every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879c42f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22849a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0a2fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_epochs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda82ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ef0c49",
   "metadata": {},
   "source": [
    "save_name = 'export/berte_pretrain_mlm_15p'\n",
    "tf.saved_model.save(pretrainer, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
