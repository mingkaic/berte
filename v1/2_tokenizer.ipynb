{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e7faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 09:31:04.576913: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-01 09:31:04.576938: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ea2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_TEXT = 'tmp/berte_per_sentence.txt'\n",
    "MODEL = 'export/berte_tokenizer'\n",
    "VOCAB_SIZE = 16000\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efb4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 09:31:06.080626: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-01 09:31:06.080654: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-01 09:31:06.080686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mingkaichen-Macmini): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "trains = tf.data.experimental.load('export/persentence_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adcec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_TEXT, 'w') as f:\n",
    "    for entry in trains:\n",
    "        f.write(entry.numpy().decode() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa9ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tmp/berte_per_sentence.txt    --model_prefix=export/berte_tokenizer    --vocab_size=16000    --pad_id=0    --unk_id=1    --bos_id=2    --eos_id=3    --control_symbols=<mask>,<sep>\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tmp/berte_per_sentence.txt\n",
      "  input_format: \n",
      "  model_prefix: export/berte_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: <mask>\n",
      "  control_symbols: <sep>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: tmp/berte_per_sentence.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 3000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 4000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (4306689), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 4306689 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <mask>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=525994290\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9516% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=90\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999516\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 4306689 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 794792 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 4306689\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 312951\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 312951 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=339131 obj=10.3052 num_tokens=513282 num_tokens/piece=1.51352\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=242939 obj=8.18778 num_tokens=506430 num_tokens/piece=2.0846\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=182194 obj=8.14444 num_tokens=539290 num_tokens/piece=2.95998\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=182180 obj=8.13617 num_tokens=539370 num_tokens/piece=2.96064\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=136632 obj=8.1497 num_tokens=602591 num_tokens/piece=4.41032\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=136632 obj=8.14489 num_tokens=602523 num_tokens/piece=4.40982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=102474 obj=8.18143 num_tokens=665954 num_tokens/piece=6.49876\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=102474 obj=8.17385 num_tokens=665873 num_tokens/piece=6.49797\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=76855 obj=8.23169 num_tokens=724939 num_tokens/piece=9.43255\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=76855 obj=8.22098 num_tokens=724854 num_tokens/piece=9.43145\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=57641 obj=8.3048 num_tokens=779017 num_tokens/piece=13.515\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=57641 obj=8.2907 num_tokens=778980 num_tokens/piece=13.5143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=43230 obj=8.39877 num_tokens=828024 num_tokens/piece=19.1539\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=43230 obj=8.38056 num_tokens=828034 num_tokens/piece=19.1542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=32422 obj=8.51482 num_tokens=874889 num_tokens/piece=26.9844\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=32422 obj=8.49197 num_tokens=874939 num_tokens/piece=26.986\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=24316 obj=8.656 num_tokens=918721 num_tokens/piece=37.7826\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=24316 obj=8.62822 num_tokens=918977 num_tokens/piece=37.7931\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=18237 obj=8.82033 num_tokens=959692 num_tokens/piece=52.6233\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=18237 obj=8.78698 num_tokens=959795 num_tokens/piece=52.629\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=8.81042 num_tokens=965804 num_tokens/piece=54.8752\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=8.80599 num_tokens=965871 num_tokens/piece=54.879\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: export/berte_tokenizer.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: export/berte_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('''--input={}\\\n",
    "    --model_prefix={}\\\n",
    "    --vocab_size={}\\\n",
    "    --pad_id=0\\\n",
    "    --unk_id=1\\\n",
    "    --bos_id=2\\\n",
    "    --eos_id=3\\\n",
    "    --control_symbols=<mask>,<sep>'''.format(\n",
    "    TRAINING_TEXT, MODEL, VOCAB_SIZE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
